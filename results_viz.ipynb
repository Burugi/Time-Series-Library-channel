{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting Results Visualization\n",
    "\n",
    "This notebook automatically loads all experimental results and provides flexible visualization options.\n",
    "\n",
    "## Workflow:\n",
    "1. **Load all results** - Automatically scan and load all available results\n",
    "2. **Configure filters** - Select specific datasets/models/settings to visualize\n",
    "3. **Visualize** - Generate comparison tables and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results\n",
    "\n",
    "Automatically scan the results directory and load all available experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def scan_and_load_all_results(base_results_path='./results'):\n    \"\"\"\n    Scan the results directory and load all available results.\n\n    Directory structure:\n    results/{DATASET}/{seq_len}_{pred_len}/{MODEL}/{setting}/\n        - metrics.npy\n        - per_channel_metrics.npy (for CD mode)\n        - scalability.npy\n        - pred.npy\n        - true.npy\n        - input.npy\n\n    Returns:\n        results_dict: Nested dictionary {dataset: {length_config: {model: {mode: {feature: {repeat: data}}}}}}\n        metrics_df: DataFrame with all metrics\n    \"\"\"\n    results_dict = {}\n    metrics_list = []\n\n    if not os.path.exists(base_results_path):\n        print(f\"Results path does not exist: {base_results_path}\")\n        return results_dict, pd.DataFrame()\n\n    # First, load channel names from results.json files\n    channel_names_cache = {}\n\n    for dataset_dir in Path(base_results_path).iterdir():\n        if not dataset_dir.is_dir():\n            continue\n        dataset_name = dataset_dir.name\n\n        for length_dir in dataset_dir.iterdir():\n            if not length_dir.is_dir():\n                continue\n            length_config = length_dir.name\n\n            # Try to find CD results.json to get channel names\n            for json_file in length_dir.glob('*_CD_results.json'):\n                try:\n                    with open(json_file, 'r') as f:\n                        cd_results = json.load(f)\n                    if 'per_channel' in cd_results:\n                        channel_names_cache[(dataset_name, length_config)] = list(cd_results['per_channel'].keys())\n                        break\n                except:\n                    pass\n\n    # Scan all datasets\n    for dataset_dir in Path(base_results_path).iterdir():\n        if not dataset_dir.is_dir():\n            continue\n\n        dataset_name = dataset_dir.name\n        results_dict[dataset_name] = {}\n\n        # Scan all length configurations\n        for length_dir in dataset_dir.iterdir():\n            if not length_dir.is_dir():\n                continue\n\n            length_config = length_dir.name\n            try:\n                seq_len, pred_len = map(int, length_config.split('_'))\n            except:\n                continue\n\n            results_dict[dataset_name][length_config] = {}\n\n            # Get channel names for this dataset/length config\n            channel_names = channel_names_cache.get((dataset_name, length_config), None)\n\n            # Scan all models\n            for model_dir in length_dir.iterdir():\n                if not model_dir.is_dir():\n                    continue\n\n                model_name = model_dir.name\n                results_dict[dataset_name][length_config][model_name] = {}\n\n                # Scan all experiment settings\n                for setting_dir in model_dir.iterdir():\n                    if not setting_dir.is_dir():\n                        continue\n\n                    setting_name = setting_dir.name\n                    parts = setting_name.split('_')\n\n                    if len(parts) < 3:\n                        continue\n\n                    # Find mode (CD or CI)\n                    mode = None\n                    mode_idx = -1\n                    for i, part in enumerate(parts):\n                        if part in ['CD', 'CI']:\n                            mode = part\n                            mode_idx = i\n                            break\n\n                    if mode is None:\n                        continue\n\n                    # Find repeat number\n                    repeat_num = None\n                    for i in range(len(parts) - 1, -1, -1):\n                        if parts[i].startswith('repeat'):\n                            repeat_num = parts[i].replace('repeat', '')\n                            break\n\n                    if repeat_num is None:\n                        continue\n\n                    # Extract feature name (for CI mode)\n                    if mode == 'CI':\n                        feature_parts = []\n                        for i in range(mode_idx + 1, len(parts)):\n                            if parts[i].startswith('repeat'):\n                                break\n                            feature_parts.append(parts[i])\n                        feature = '_'.join(feature_parts) if feature_parts else 'unknown'\n                    else:\n                        feature = 'all'\n\n                    # Initialize nested structure\n                    if mode not in results_dict[dataset_name][length_config][model_name]:\n                        results_dict[dataset_name][length_config][model_name][mode] = {}\n\n                    if feature not in results_dict[dataset_name][length_config][model_name][mode]:\n                        results_dict[dataset_name][length_config][model_name][mode][feature] = {}\n\n                    # Load data files\n                    metrics_path = setting_dir / 'metrics.npy'\n                    per_channel_metrics_path = setting_dir / 'per_channel_metrics.npy'\n                    scalability_path = setting_dir / 'scalability.npy'\n                    pred_path = setting_dir / 'pred.npy'\n                    true_path = setting_dir / 'true.npy'\n                    input_path = setting_dir / 'input.npy'\n\n                    if not all([metrics_path.exists(), pred_path.exists(), true_path.exists()]):\n                        continue\n\n                    try:\n                        metrics = np.load(str(metrics_path))\n                        pred = np.load(str(pred_path))\n                        true = np.load(str(true_path))\n                        input_seq = np.load(str(input_path)) if input_path.exists() else None\n                        per_channel_metrics = np.load(str(per_channel_metrics_path)) if per_channel_metrics_path.exists() else None\n                        scalability = np.load(str(scalability_path)) if scalability_path.exists() else None\n\n                        results_dict[dataset_name][length_config][model_name][mode][feature][repeat_num] = {\n                            'metrics': metrics,\n                            'pred': pred,\n                            'true': true,\n                            'input': input_seq,\n                            'per_channel_metrics': per_channel_metrics,\n                            'scalability': scalability\n                        }\n\n                        # Add overall metrics to list\n                        base_metrics = {\n                            'Dataset': dataset_name,\n                            'Seq_Len': seq_len,\n                            'Pred_Len': pred_len,\n                            'Model': model_name,\n                            'Mode': mode,\n                            'Feature': feature,\n                            'Channel': 'overall',\n                            'Repeat': repeat_num,\n                            'MAE': metrics[0],\n                            'MSE': metrics[1],\n                            'RMSE': metrics[2],\n                            'MAPE': metrics[3],\n                            'MSPE': metrics[4]\n                        }\n\n                        # Add scalability metrics if available\n                        if scalability is not None:\n                            base_metrics['Inference_Time'] = scalability[0]\n                            base_metrics['Inference_Memory_GB'] = scalability[1]\n\n                        metrics_list.append(base_metrics)\n\n                        # Add per-channel metrics for CD mode\n                        if mode == 'CD' and per_channel_metrics is not None and channel_names is not None:\n                            n_channels = per_channel_metrics.shape[0]\n                            for ch in range(min(n_channels, len(channel_names))):\n                                ch_metrics = {\n                                    'Dataset': dataset_name,\n                                    'Seq_Len': seq_len,\n                                    'Pred_Len': pred_len,\n                                    'Model': model_name,\n                                    'Mode': mode,\n                                    'Feature': feature,\n                                    'Channel': channel_names[ch],\n                                    'Repeat': repeat_num,\n                                    'MAE': per_channel_metrics[ch, 0],\n                                    'MSE': per_channel_metrics[ch, 1],\n                                    'RMSE': per_channel_metrics[ch, 2],\n                                    'MAPE': per_channel_metrics[ch, 3],\n                                    'MSPE': per_channel_metrics[ch, 4]\n                                }\n\n                                # Scalability is same for all channels\n                                if scalability is not None:\n                                    ch_metrics['Inference_Time'] = scalability[0]\n                                    ch_metrics['Inference_Memory_GB'] = scalability[1]\n\n                                metrics_list.append(ch_metrics)\n                    except Exception as e:\n                        print(f\"Error loading {setting_dir}: {e}\")\n                        continue\n\n    metrics_df = pd.DataFrame(metrics_list)\n    return results_dict, metrics_df\n\n# Load all results\nprint(\"Scanning results directory...\")\nresults_dict, all_metrics_df = scan_and_load_all_results('./results')\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Loaded {len(all_metrics_df)} experimental results\")\nprint(f\"{'='*80}\\n\")\n\nif len(all_metrics_df) > 0:\n    print(\"Available configurations:\")\n    print(f\"\\nDatasets: {all_metrics_df['Dataset'].unique().tolist()}\")\n    print(f\"\\nLength configurations (Seq_Len, Pred_Len):\")\n    for _, row in all_metrics_df[['Seq_Len', 'Pred_Len']].drop_duplicates().iterrows():\n        print(f\"  - {row['Seq_Len']} → {row['Pred_Len']}\")\n    print(f\"\\nModels: {all_metrics_df['Model'].unique().tolist()}\")\n    print(f\"\\nModes: {all_metrics_df['Mode'].unique().tolist()}\")\n    print(f\"\\nFeatures (CI mode): {[f for f in all_metrics_df['Feature'].unique() if f != 'all']}\")\n    print(f\"\\nTotal experiments by configuration:\")\n    print(all_metrics_df.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature', 'Channel']).size())\nelse:\n    print(\"No results found! Please run experiments first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metrics Summary Table\n",
    "\n",
    "Display summary statistics for filtered results.\n",
    "\n",
    "**Note**: For CI mode, metrics are shown both per-feature and as an average across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(all_metrics_df) > 0:\n    # ========== CD Overall Summary ==========\n    print(\"\\n\" + \"=\"*100)\n    print(\"CD MODE: OVERALL METRICS (Mean ± Std across repeats)\")\n    print(\"=\"*100)\n    \n    cd_overall = all_metrics_df[(all_metrics_df['Mode'] == 'CD') & (all_metrics_df['Channel'] == 'overall')]\n    if len(cd_overall) > 0:\n        cd_overall_summary = cd_overall.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']).agg({\n            'MAE': ['mean', 'std'],\n            'MSE': ['mean', 'std'],\n            'RMSE': ['mean', 'std'],\n            'MAPE': ['mean', 'std'],\n            'MSPE': ['mean', 'std']\n        }).round(4)\n        print(cd_overall_summary)\n    else:\n        print(\"No CD overall results found.\")\n    \n    # ========== CD Per-Channel Summary ==========\n    print(\"\\n\" + \"=\"*100)\n    print(\"CD MODE: PER-CHANNEL METRICS (Mean ± Std across repeats)\")\n    print(\"=\"*100)\n    \n    cd_channels = all_metrics_df[(all_metrics_df['Mode'] == 'CD') & (all_metrics_df['Channel'] != 'overall')]\n    if len(cd_channels) > 0:\n        cd_channel_summary = cd_channels.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature', 'Channel']).agg({\n            'MAE': ['mean', 'std'],\n            'MSE': ['mean', 'std'],\n            'RMSE': ['mean', 'std'],\n            'MAPE': ['mean', 'std'],\n            'MSPE': ['mean', 'std']\n        }).round(4)\n        print(cd_channel_summary)\n    else:\n        print(\"No CD per-channel results found.\")\n    \n    # ========== CI Per-Feature Summary ==========\n    print(\"\\n\" + \"=\"*100)\n    print(\"CI MODE: PER-FEATURE METRICS (Mean ± Std across repeats)\")\n    print(\"=\"*100)\n    \n    ci_data = all_metrics_df[all_metrics_df['Mode'] == 'CI']\n    if len(ci_data) > 0:\n        ci_summary = ci_data.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']).agg({\n            'MAE': ['mean', 'std'],\n            'MSE': ['mean', 'std'],\n            'RMSE': ['mean', 'std'],\n            'MAPE': ['mean', 'std'],\n            'MSPE': ['mean', 'std']\n        }).round(4)\n        print(ci_summary)\n    else:\n        print(\"No CI results found.\")\n    \n    # ========== CI Average Calculation ==========\n    if len(ci_data) > 0:\n        print(\"\\n\" + \"=\"*100)\n        print(\"CI MODE: AVERAGE ACROSS ALL FEATURES\")\n        print(\"=\"*100)\n        \n        # Calculate average metrics across features for each model\n        ci_avg_list = []\n        for (dataset, seq_len, pred_len, model), group in ci_data.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model']):\n            # Average across all features and repeats\n            ci_avg_list.append({\n                'Dataset': dataset,\n                'Seq_Len': seq_len,\n                'Pred_Len': pred_len,\n                'Model': model,\n                'Mode': 'CI_avg',\n                'Feature': 'all_features_avg',\n                'MAE_mean': group['MAE'].mean(),\n                'MAE_std': group['MAE'].std(),\n                'MSE_mean': group['MSE'].mean(),\n                'MSE_std': group['MSE'].std(),\n                'RMSE_mean': group['RMSE'].mean(),\n                'RMSE_std': group['RMSE'].std(),\n                'MAPE_mean': group['MAPE'].mean(),\n                'MAPE_std': group['MAPE'].std(),\n                'MSPE_mean': group['MSPE'].mean(),\n                'MSPE_std': group['MSPE'].std()\n            })\n        \n        ci_avg_df = pd.DataFrame(ci_avg_list)\n        print(ci_avg_df.to_string(index=False))\n    \n    # ========== Simplified Comparison Table ==========\n    print(\"\\n\" + \"=\"*100)\n    print(\"SIMPLIFIED COMPARISON TABLE\")\n    print(\"=\"*100)\n    \n    comparison_data = []\n    \n    # Add CD overall results\n    if len(cd_overall) > 0:\n        for idx, row in cd_overall.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']):\n            comparison_data.append({\n                'Dataset': idx[0],\n                'Lengths': f\"{idx[1]}→{idx[2]}\",\n                'Model': idx[3],\n                'Mode': idx[4],\n                'Feature': idx[5],\n                'Channel': 'overall',\n                'MAE': f\"{row['MAE'].mean():.4f}±{row['MAE'].std():.4f}\",\n                'RMSE': f\"{row['RMSE'].mean():.4f}±{row['RMSE'].std():.4f}\",\n                'MAPE': f\"{row['MAPE'].mean():.4f}±{row['MAPE'].std():.4f}\"\n            })\n    \n    # Add CD per-channel results\n    if len(cd_channels) > 0:\n        for idx, row in cd_channels.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature', 'Channel']):\n            comparison_data.append({\n                'Dataset': idx[0],\n                'Lengths': f\"{idx[1]}→{idx[2]}\",\n                'Model': idx[3],\n                'Mode': idx[4],\n                'Feature': idx[5],\n                'Channel': idx[6],\n                'MAE': f\"{row['MAE'].mean():.4f}±{row['MAE'].std():.4f}\",\n                'RMSE': f\"{row['RMSE'].mean():.4f}±{row['RMSE'].std():.4f}\",\n                'MAPE': f\"{row['MAPE'].mean():.4f}±{row['MAPE'].std():.4f}\"\n            })\n    \n    # Add CI per-feature results\n    if len(ci_data) > 0:\n        for idx, row in ci_data.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']):\n            comparison_data.append({\n                'Dataset': idx[0],\n                'Lengths': f\"{idx[1]}→{idx[2]}\",\n                'Model': idx[3],\n                'Mode': idx[4],\n                'Feature': idx[5],\n                'Channel': 'N/A',\n                'MAE': f\"{row['MAE'].mean():.4f}±{row['MAE'].std():.4f}\",\n                'RMSE': f\"{row['RMSE'].mean():.4f}±{row['RMSE'].std():.4f}\",\n                'MAPE': f\"{row['MAPE'].mean():.4f}±{row['MAPE'].std():.4f}\"\n            })\n        \n        # Add CI average results\n        for _, row in ci_avg_df.iterrows():\n            comparison_data.append({\n                'Dataset': row['Dataset'],\n                'Lengths': f\"{row['Seq_Len']}→{row['Pred_Len']}\",\n                'Model': row['Model'],\n                'Mode': 'CI_avg',\n                'Feature': 'all_features_avg',\n                'Channel': 'N/A',\n                'MAE': f\"{row['MAE_mean']:.4f}±{row['MAE_std']:.4f}\",\n                'RMSE': f\"{row['RMSE_mean']:.4f}±{row['RMSE_std']:.4f}\",\n                'MAPE': f\"{row['MAPE_mean']:.4f}±{row['MAPE_std']:.4f}\"\n            })\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    print(comparison_df.to_string(index=False))\n    \nelse:\n    print(\"\\nNo filtered metrics to display.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration for Visualization\n",
    "\n",
    "Set filters to select which results to visualize. Set to `None` to include all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VISUALIZATION CONFIGURATION ==========\n",
    "\n",
    "# Filter by dataset (None for all datasets)\n",
    "FILTER_DATASET = 'milano_6165'  # or None\n",
    "\n",
    "# Filter by sequence lengths (None for all lengths)\n",
    "FILTER_SEQ_LEN = 96  # or None\n",
    "FILTER_PRED_LEN = 96  # or None\n",
    "\n",
    "# Filter by models (None for all models)\n",
    "FILTER_MODELS = ['Autoformer', 'SegRNN', 'TimeMixer', 'SCINet']  # or None\n",
    "\n",
    "# Filter by modes (None for all modes)\n",
    "FILTER_MODES = ['CD', 'CI']  # or None\n",
    "\n",
    "# For CI mode, filter by features (None for all features)\n",
    "FILTER_CI_FEATURES = None  # e.g., ['OT', 'smsin'] or None\n",
    "\n",
    "# Number of samples to plot in detailed visualization\n",
    "N_SAMPLES_TO_PLOT = 3\n",
    "\n",
    "# Random seed for sample selection\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = all_metrics_df.copy()\n",
    "\n",
    "if FILTER_DATASET is not None:\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == FILTER_DATASET]\n",
    "\n",
    "if FILTER_SEQ_LEN is not None:\n",
    "    filtered_df = filtered_df[filtered_df['Seq_Len'] == FILTER_SEQ_LEN]\n",
    "\n",
    "if FILTER_PRED_LEN is not None:\n",
    "    filtered_df = filtered_df[filtered_df['Pred_Len'] == FILTER_PRED_LEN]\n",
    "\n",
    "if FILTER_MODELS is not None:\n",
    "    filtered_df = filtered_df[filtered_df['Model'].isin(FILTER_MODELS)]\n",
    "\n",
    "if FILTER_MODES is not None:\n",
    "    filtered_df = filtered_df[filtered_df['Mode'].isin(FILTER_MODES)]\n",
    "\n",
    "if FILTER_CI_FEATURES is not None:\n",
    "    # Keep all CD results and only specified CI features\n",
    "    filtered_df = filtered_df[\n",
    "        (filtered_df['Mode'] == 'CD') | \n",
    "        ((filtered_df['Mode'] == 'CI') & (filtered_df['Feature'].isin(FILTER_CI_FEATURES)))\n",
    "    ]\n",
    "\n",
    "print(f\"Filtered to {len(filtered_df)} results\")\n",
    "print(f\"\\nFiltered configurations:\")\n",
    "if len(filtered_df) > 0:\n",
    "    print(filtered_df.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']).size())\n",
    "else:\n",
    "    print(\"No results match the filters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Detailed Prediction Visualization with Input Sequence\n\nPlot input sequence, ground truth, and predictions together."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_input(results_dict, dataset, length_config, model, mode, feature='all', repeat='0', n_samples=3):\n",
    "    \"\"\"\n",
    "    Plot input, predictions, and ground truth for a specific configuration.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = results_dict[dataset][length_config][model][mode][feature][repeat]\n",
    "        pred = data['pred']\n",
    "        true = data['true']\n",
    "        input_seq = data['input']\n",
    "        \n",
    "        if input_seq is None:\n",
    "            print(f\"Warning: No input sequence saved for {dataset}/{length_config}/{model}/{mode}/{feature}\")\n",
    "            return\n",
    "        \n",
    "        # Select random samples to plot\n",
    "        n_total = pred.shape[0]\n",
    "        n_samples = min(n_samples, n_total)\n",
    "        sample_indices = np.random.choice(n_total, n_samples, replace=False)\n",
    "        \n",
    "        # Determine number of features\n",
    "        n_features = pred.shape[2]\n",
    "        seq_len = input_seq.shape[1]\n",
    "        pred_len = pred.shape[1]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(n_samples, n_features, figsize=(7*n_features, 4*n_samples))\n",
    "        if n_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        if n_features == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        fig.suptitle(\n",
    "            f'{dataset} - {model} - {mode} - {feature} (Repeat {repeat})\\n'\n",
    "            f'Input: {seq_len} steps → Prediction: {pred_len} steps',\n",
    "            fontsize=16, y=0.998\n",
    "        )\n",
    "        \n",
    "        for i, sample_idx in enumerate(sample_indices):\n",
    "            for j in range(n_features):\n",
    "                ax = axes[i, j]\n",
    "                \n",
    "                # Time steps\n",
    "                input_time = np.arange(seq_len)\n",
    "                output_time = np.arange(seq_len, seq_len + pred_len)\n",
    "                \n",
    "                # Plot input sequence\n",
    "                ax.plot(input_time, input_seq[sample_idx, :, j], \n",
    "                       label='Input Sequence', color='gray', linewidth=2, alpha=0.7)\n",
    "                \n",
    "                # Plot ground truth\n",
    "                ax.plot(output_time, true[sample_idx, :, j], \n",
    "                       label='Ground Truth', color='blue', linewidth=2, alpha=0.7)\n",
    "                \n",
    "                # Plot prediction\n",
    "                ax.plot(output_time, pred[sample_idx, :, j], \n",
    "                       label='Prediction', color='red', linewidth=2, linestyle='--', alpha=0.7)\n",
    "                \n",
    "                # Add vertical line to separate input and output\n",
    "                ax.axvline(x=seq_len, color='black', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "                ax.text(seq_len, ax.get_ylim()[1], ' Forecast Start', \n",
    "                       fontsize=9, verticalalignment='top')\n",
    "                \n",
    "                ax.set_xlabel('Time Step', fontsize=11)\n",
    "                ax.set_ylabel('Value', fontsize=11)\n",
    "                ax.set_title(f'Sample {sample_idx} - Feature {j}', fontsize=12)\n",
    "                ax.legend(loc='best', fontsize=10)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Configuration not found - {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting: {e}\")\n",
    "\n",
    "# Plot predictions for a sample of filtered configurations\n",
    "if len(filtered_df) > 0:\n",
    "    # Get unique configurations from filtered data\n",
    "    configs = filtered_df.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model', 'Mode', 'Feature']).size().reset_index()\n",
    "    \n",
    "    # Limit to first 3 configurations to avoid too many plots\n",
    "    for _, config in configs.head(3).iterrows():\n",
    "        dataset = config['Dataset']\n",
    "        length_config = f\"{config['Seq_Len']}_{config['Pred_Len']}\"\n",
    "        model = config['Model']\n",
    "        mode = config['Mode']\n",
    "        feature = config['Feature']\n",
    "        \n",
    "        # Check if this configuration exists in results_dict\n",
    "        try:\n",
    "            if (dataset in results_dict and \n",
    "                length_config in results_dict[dataset] and\n",
    "                model in results_dict[dataset][length_config] and\n",
    "                mode in results_dict[dataset][length_config][model] and\n",
    "                feature in results_dict[dataset][length_config][model][mode]):\n",
    "                \n",
    "                # Get first repeat for visualization\n",
    "                repeats = list(results_dict[dataset][length_config][model][mode][feature].keys())\n",
    "                if repeats:\n",
    "                    repeat = repeats[0]\n",
    "                    print(f\"\\nPlotting: {dataset} - {length_config} - {model} - {mode} - {feature}\")\n",
    "                    plot_predictions_with_input(\n",
    "                        results_dict, dataset, length_config, model, mode, feature, repeat, N_SAMPLES_TO_PLOT\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing configuration: {e}\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"No results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Export Summary to CSV\n\nSave the metrics summary to CSV files for further analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Scalability Analysis\n\nAnalyze and compare scalability metrics (training time, inference time, GPU memory usage) across models and modes."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "if len(filtered_df) > 0 and 'Inference_Time' in filtered_df.columns:\n    # Calculate scalability statistics\n    print(\"\\n\" + \"=\"*100)\n    print(\"SCALABILITY METRICS SUMMARY\")\n    print(\"=\"*100)\n\n    # Group by Model and Mode\n    scalability_summary = filtered_df[filtered_df['Channel'] == 'overall'].groupby(['Dataset', 'Model', 'Mode']).agg({\n        'Inference_Time': ['mean', 'std'],\n        'Inference_Memory_GB': ['mean', 'std']\n    }).round(4)\n\n    print(scalability_summary)\n\n    # Visualization\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n    # Get data for plotting\n    plot_data = filtered_df[filtered_df['Channel'] == 'overall'].copy()\n\n    # 1. Inference Time Comparison\n    ax = axes[0, 0]\n    models = plot_data['Model'].unique()\n    x = np.arange(len(models))\n    width = 0.35\n\n    cd_times = []\n    ci_times = []\n    for model in models:\n        cd_val = plot_data[(plot_data['Model'] == model) & (plot_data['Mode'] == 'CD')]['Inference_Time']\n        ci_val = plot_data[(plot_data['Model'] == model) & (plot_data['Mode'] == 'CI')]['Inference_Time']\n        cd_times.append(cd_val.mean() if len(cd_val) > 0 else 0)\n        ci_times.append(ci_val.mean() if len(ci_val) > 0 else 0)\n\n    bars1 = ax.bar(x - width/2, cd_times, width, label='CD', alpha=0.8, color='skyblue')\n    bars2 = ax.bar(x + width/2, ci_times, width, label='CI', alpha=0.8, color='lightcoral')\n\n    ax.set_ylabel('Inference Time (s)', fontsize=12)\n    ax.set_title('Inference Time Comparison', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(models, rotation=45, ha='right')\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n\n    # 2. Inference Memory Comparison\n    ax = axes[0, 1]\n    cd_memory = []\n    ci_memory = []\n    for model in models:\n        cd_val = plot_data[(plot_data['Model'] == model) & (plot_data['Mode'] == 'CD')]['Inference_Memory_GB']\n        ci_val = plot_data[(plot_data['Model'] == model) & (plot_data['Mode'] == 'CI')]['Inference_Memory_GB']\n        cd_memory.append(cd_val.mean() if len(cd_val) > 0 else 0)\n        ci_memory.append(ci_val.mean() if len(ci_val) > 0 else 0)\n\n    bars1 = ax.bar(x - width/2, cd_memory, width, label='CD', alpha=0.8, color='skyblue')\n    bars2 = ax.bar(x + width/2, ci_memory, width, label='CI', alpha=0.8, color='lightcoral')\n\n    ax.set_ylabel('GPU Memory (GB)', fontsize=12)\n    ax.set_title('Inference Memory Comparison', fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels(models, rotation=45, ha='right')\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n\n    # 3. Inference Time vs MAE Trade-off\n    ax = axes[1, 0]\n    for mode in ['CD', 'CI']:\n        mode_data = plot_data[plot_data['Mode'] == mode]\n        if len(mode_data) > 0:\n            ax.scatter(mode_data['Inference_Time'], mode_data['MAE'],\n                      label=mode, s=100, alpha=0.6)\n\n            # Add model labels\n            for _, row in mode_data.iterrows():\n                ax.annotate(row['Model'],\n                           (row['Inference_Time'], row['MAE']),\n                           fontsize=8, alpha=0.7)\n\n    ax.set_xlabel('Inference Time (s)', fontsize=12)\n    ax.set_ylabel('MAE', fontsize=12)\n    ax.set_title('Inference Time vs MAE Trade-off', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # 4. Memory vs MAE Trade-off\n    ax = axes[1, 1]\n    for mode in ['CD', 'CI']:\n        mode_data = plot_data[plot_data['Mode'] == mode]\n        if len(mode_data) > 0:\n            ax.scatter(mode_data['Inference_Memory_GB'], mode_data['MAE'],\n                      label=mode, s=100, alpha=0.6)\n\n            # Add model labels\n            for _, row in mode_data.iterrows():\n                ax.annotate(row['Model'],\n                           (row['Inference_Memory_GB'], row['MAE']),\n                           fontsize=8, alpha=0.7)\n\n    ax.set_xlabel('GPU Memory (GB)', fontsize=12)\n    ax.set_ylabel('MAE', fontsize=12)\n    ax.set_title('Memory vs MAE Trade-off', fontsize=14, fontweight='bold')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Print detailed comparison table\n    print(\"\\n\" + \"=\"*100)\n    print(\"SCALABILITY DETAILED COMPARISON\")\n    print(\"=\"*100)\n\n    comparison_data = []\n    for idx, row in plot_data.groupby(['Dataset', 'Model', 'Mode']):\n        comparison_data.append({\n            'Dataset': idx[0],\n            'Model': idx[1],\n            'Mode': idx[2],\n            'Inference_Time(s)': f\"{row['Inference_Time'].mean():.4f}±{row['Inference_Time'].std():.4f}\",\n            'Inference_Memory(GB)': f\"{row['Inference_Memory_GB'].mean():.4f}±{row['Inference_Memory_GB'].std():.4f}\",\n            'MAE': f\"{row['MAE'].mean():.4f}±{row['MAE'].std():.4f}\",\n            'RMSE': f\"{row['RMSE'].mean():.4f}±{row['RMSE'].std():.4f}\"\n        })\n\n    comparison_df_scalability = pd.DataFrame(comparison_data)\n    print(comparison_df_scalability.to_string(index=False))\nelse:\n    print(\"No scalability metrics available or no filtered data.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(filtered_df) > 0:\n    # Create filename based on filters\n    filename_parts = ['results_summary']\n    if FILTER_DATASET:\n        filename_parts.append(FILTER_DATASET)\n    if FILTER_SEQ_LEN and FILTER_PRED_LEN:\n        filename_parts.append(f\"{FILTER_SEQ_LEN}_{FILTER_PRED_LEN}\")\n    \n    base_filename = '_'.join(filename_parts)\n    \n    # Save detailed metrics\n    detailed_output_path = f'./{base_filename}_detailed.csv'\n    filtered_df.to_csv(detailed_output_path, index=False)\n    print(f\"Detailed metrics saved to: {detailed_output_path}\")\n    \n    # Save comparison table\n    if 'comparison_df' in locals() and len(comparison_df) > 0:\n        comparison_output_path = f'./{base_filename}_comparison.csv'\n        comparison_df.to_csv(comparison_output_path, index=False)\n        print(f\"Comparison table saved to: {comparison_output_path}\")\n    \n    # Save overall summary (CD Overall + CI Average) for paper_viz\n    overall_summary_list = []\n    \n    # Add CD overall results\n    cd_overall = all_metrics_df[(all_metrics_df['Mode'] == 'CD') & (all_metrics_df['Channel'] == 'overall')]\n    if len(cd_overall) > 0:\n        cd_summary = cd_overall.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model']).agg({\n            'MAE': 'mean',\n            'MSE': 'mean',\n            'RMSE': 'mean',\n            'MAPE': 'mean',\n            'MSPE': 'mean',\n            'Inference_Time': 'mean',\n            'Inference_Memory_GB': 'mean'\n        }).reset_index()\n        cd_summary['Mode'] = 'CD'\n        overall_summary_list.append(cd_summary)\n    \n    # Add CI average results\n    ci_data = all_metrics_df[all_metrics_df['Mode'] == 'CI']\n    if len(ci_data) > 0:\n        ci_avg = ci_data.groupby(['Dataset', 'Seq_Len', 'Pred_Len', 'Model']).agg({\n            'MAE': 'mean',\n            'MSE': 'mean',\n            'RMSE': 'mean',\n            'MAPE': 'mean',\n            'MSPE': 'mean',\n            'Inference_Time': 'mean',\n            'Inference_Memory_GB': 'mean'\n        }).reset_index()\n        ci_avg['Mode'] = 'CI'\n        overall_summary_list.append(ci_avg)\n    \n    # Combine and save\n    if overall_summary_list:\n        overall_summary_df = pd.concat(overall_summary_list, ignore_index=True)\n        overall_output_path = './results_summary.csv'\n        overall_summary_df.to_csv(overall_output_path, index=False)\n        print(f\"Overall summary saved to: {overall_output_path}\")\n        print(f\"  - Includes {len(overall_summary_df[overall_summary_df['Mode'] == 'CD'])} CD results\")\n        print(f\"  - Includes {len(overall_summary_df[overall_summary_df['Mode'] == 'CI'])} CI results\")\nelse:\n    print(\"No metrics to export.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}